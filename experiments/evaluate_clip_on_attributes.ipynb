{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import clip\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "from argparse import Namespace\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from viscoin.datasets.cub import Labeled_CUB_200_2011\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"\"), \"./../clip/LoRA/\"))\n",
    "\n",
    "from loralib.utils import (\n",
    "    apply_lora,\n",
    "    load_lora,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clip model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets/CUB_200_2011/\"\n",
    "dataset = Labeled_CUB_200_2011(dataset_path, mode='test', transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(**{\n",
    "    # Model backbone type (e.g., CLIP backbone)\n",
    "    'backbone': 'ViT-B/16',  # Default: 'ViT-B/16'\n",
    "\n",
    "    # LoRA-specific arguments\n",
    "    # Defines where to insert LoRA modules within the model's layers\n",
    "    'position': 'all',  # Options: 'bottom', 'mid', 'up', 'half-up', 'half-bottom', 'all', 'top3'; Default: 'all'\n",
    "    # Determines whether to apply LoRA to the text encoder, vision encoder, or both\n",
    "    'encoder': 'both',  # Options: 'text', 'vision', 'both'; Default: 'both'\n",
    "    # Specifies which attention matrices in the model will use LoRA\n",
    "    'params': ('q', 'k', 'v'),  # Default: Query (q), Key (k), and Value (v) matrices\n",
    "    # The rank of the low-rank matrices used in LoRA\n",
    "    'r': 2,  # Default: 2\n",
    "    # Scaling factor applied to LoRA matrices (see LoRA paper for details)\n",
    "    'alpha': 1,  # Default: 1\n",
    "    # Dropout rate applied before the LoRA module\n",
    "    'dropout_rate': 0.25,  # Default: 0.25\n",
    "\n",
    "    # Filename for saving LoRA weights (without extension)\n",
    "    'filename': 'lora_weights',  # Default: 'lora_weights'    \n",
    "    # Path to save the LoRA weights after training; will not save if set to None\n",
    "    'save_path': \"./../clip/LoRA/weights/\"  # Default: './../clip/LoRA/weights/'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA weights loaded from ./../clip/LoRA/weights//lora_weights.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/hugo.aoyagi/PRIM/VisCoIN/experiments/./../clip/LoRA/loralib/utils.py:281: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(load_path)\n"
     ]
    }
   ],
   "source": [
    "list_lora_layers = apply_lora(args, model)\n",
    "load_lora(args, list_lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_float32(model):\n",
    "    \"\"\"Converts all weights in a PyTorch model to float32 precision.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dtype == torch.float16:  # Check if the parameter is in half-precision\n",
    "            param.data = param.data.float()  # Convert the parameter data to float32\n",
    "        if param.grad is not None and param.grad.dtype == torch.float16:  # Convert gradient, if it exists\n",
    "            param.grad.data = param.grad.data.float()\n",
    "    return model\n",
    "\n",
    "model = convert_model_to_float32(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "There are 312 possible attributes in the dataset : \"blue wings\", \"red belly\" ...etc.\n",
    "We want clip to rank the probabilities of each of those attributes for each image and we compare the top attributes with the attributes actually present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipAttributeClassifier:\n",
    "    def __init__(self, model, preprocess, dataset, device):\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get attribute labels\n",
    "        self.attribute_labels = list(dataset.attributes_labels.values())\n",
    "        self.attribute_labels = [f\"A photo of a bird with {dataset.get_attribute_caption(attr)}\" for attr in self.attribute_labels]\n",
    "        \n",
    "        # Get text features\n",
    "        self.text_inputs = clip.tokenize(self.attribute_labels).to(device)\n",
    "            \n",
    "    def classify_batch(self, batch_indices):\n",
    "        \"\"\"\n",
    "        Classify a batch of images, predicting the top-k attributes for each image where k is the number of attributes for a given image.\n",
    "        \n",
    "        Args:\n",
    "            batch_indices (list): Indices of the images in the dataset.\n",
    "            \n",
    "        Returns:\n",
    "            list: Accuracies for each image in the batch.\n",
    "        \"\"\"\n",
    "        images = torch.stack([self.dataset[i][0] for i in batch_indices]).to(self.device)\n",
    "        targets = [self.dataset.attributes[i] for i in batch_indices]\n",
    "        \n",
    "        # Get k values for each image in the batch\n",
    "        ks = [target.shape[0] for target in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(images)\n",
    "            text_features = self.model.encode_text(self.text_inputs)\n",
    "            \n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            logits_per_image = image_features @ text_features.T\n",
    "            probs = logits_per_image.softmax(dim=-1)   \n",
    "        \n",
    "        accs = []\n",
    "        for i, k in enumerate(ks):\n",
    "            top_indices = torch.topk(probs[i], k).indices\n",
    "\n",
    "            common_indices = set(top_indices.cpu().numpy()).intersection(set(targets[i]))\n",
    "            \n",
    "            accs.append(len(common_indices) / k)\n",
    "        \n",
    "        return accs\n",
    "    \n",
    "    def average_accuracy(self, batch_size=32, n=1000):\n",
    "        \"\"\"\n",
    "        Compute average accuracy over the dataset in batches.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of images per batch.\n",
    "            n (int): Maximum number of images to evaluate.\n",
    "            \n",
    "        Returns:\n",
    "            float: Average accuracy.\n",
    "        \"\"\"\n",
    "        num_samples = min(n, len(self.dataset))\n",
    "        accs = []\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "            batch_indices = list(range(i, min(i + batch_size, num_samples)))\n",
    "            accs.extend(self.classify_batch(batch_indices))\n",
    "        \n",
    "        return np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0853576100637068)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClipAttributeClassifier(model, preprocess, dataset, device)\n",
    "\n",
    "classifier.average_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viscoin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
